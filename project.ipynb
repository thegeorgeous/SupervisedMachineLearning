{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Final Project\n",
    "\n",
    "Diabetes is a chronic condition that occurs when the pancreas can no longer make insulin, or the body cannot effectively use insulin. Insulin is a hormone created in the pancreas that to move glucose into cells in the body. Glucose is a source of energy for the body. A person with diabetes is unable to control the levels of glucose in their blood. High levels of glucose over a long term is associated with organ failure and other damage to the body. As per [this report](https://idf.org/media/uploads/2024/06/IDF-Annual-Report-2023.pdf) from the International Diabetes Federation, an estimated 540 million people live with diabetes.\n",
    "\n",
    "Early diabetes detection and prediction is an important tool to prevent diabetes. While there are several modern tests/features that can be used for diabetes detection, we will use some of the historically common ones that are cheaper and readily available. e.g. BMI, Glucose, Insulin, Age etc.\n",
    "\n",
    "New research has shown that data preprocessed with Recursive Feature Elimination(RFE) and methods like eXtreme Gradient Boosting(XGBoost) and Stacking can provide a higher performance compared to Random Forest classifiers. The goal of these project is to explore the improvement in accuracy with these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib seaborn scikit-learn xgboost kagglehub docutils\n",
    "\n",
    "import kagglehub\n",
    "import docutils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "For this project, we will use the Pima Indians Diabetes Dataset made available by UCI Machine Learning on Kaggle. This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version\n",
    "data = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"uciml/pima-indians-diabetes-database\",\n",
    "  \"diabetes.csv\",\n",
    ")\n",
    "print(data.head())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "The small number of records and the constraints mentioned in the previous section can skew the data and affect the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find distribution of target variable\n",
    "data.hist(figsize = (10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling zero values\n",
    "As shown on the graph, several features in the dataset have an impossible zero value. e.g. Glucose, Blood Pressure and BMI. These seem to be missing values that need to be imputed. We will impute these values using the median value of these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zero values with median\n",
    "zero_columns = ['Glucose', 'BloodPressure', 'BMI', 'SkinThickness', 'Insulin']\n",
    "for col in zero_columns:\n",
    "    data[col] = data[col].replace(0, data[col].median())\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histplot with cleaned data\n",
    "data.hist(figsize = (10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, hue='Outcome', diag_kind='hist', height=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairplot insights\n",
    "- A counterintuitive finding is that the Glucose and Insulin are not linearly related.\n",
    "- The pairplot also shows that BMI and Skin Thickness are correlated and therefore only one of them might be necessary for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data.corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heatmap insights\n",
    "- Outcomes are highly correlated to Glucose, BMI and Age. Intuitively, these should be the top three predictors for the models.\n",
    "- Insulin, Blood Pressure and DiabetesPedigreeFunction are poor predictors becuase of their low correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying the models, we need to scale the data to ensure that the features are on the same scale before they are selected using a Feature Selector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination using Logistic Regression as the base estimator\n",
    "We will now use the RFE available in scikit-learn to select 4 features using recursive feature elimination. The features that provide the highest accuracy will be chosen. The estimator used is Logistic Regression. We will then create a training and test sample that is derived based on these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(random_state=42)\n",
    "rfe = RFE(estimator=lr_model, n_features_to_select=4, step=1)\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = X.columns[rfe.support_]\n",
    "print(\"Selected Features by RFE:\", selected_features)\n",
    "\n",
    "# Create new datasets with only the selected features\n",
    "X_train_rfe = rfe.transform(X_train_scaled)\n",
    "X_test_rfe = rfe.transform(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest without Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, scoring='accuracy')\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "rf_best = rf_grid.best_estimator_\n",
    "\n",
    "print(\"\\nBest Random Forest Parameters (without feature selection):\", rf_grid.best_params_)\n",
    "print(\"Random Forest Performance (without feature selection):\")\n",
    "print(classification_report(y_test, rf_best.predict(X_test_scaled)))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, rf_best.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_sfs_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, scoring='accuracy')\n",
    "rf_sfs_grid.fit(X_train_sfs, y_train)\n",
    "rf_sfs_best = rf_sfs_grid.best_estimator_\n",
    "\n",
    "print(\"\\nBest Random Forest Parameters (with feature selection):\", rf_sfs_grid.best_params_)\n",
    "print(\"Random Forest Performance (with feature selection):\")\n",
    "print(classification_report(y_test, rf_sfs_best.predict(X_test_sfs)))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, rf_sfs_best.predict(X_test_sfs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that is widely used to solve many data science problems in a fast and accurate way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3]\n",
    "}\n",
    "xgb_grid = GridSearchCV(XGBClassifier(random_state=42), xgb_params, cv=5, scoring='accuracy')\n",
    "xgb_grid.fit(X_train_scaled, y_train)\n",
    "xgb_best = xgb_grid.best_estimator_\n",
    "\n",
    "print(\"\\nBest XGBoost Parameters:\", xgb_grid.best_params_)\n",
    "print(\"XGBoost Performance:\")\n",
    "print(classification_report(y_test, xgb_best.predict(X_test_scaled)))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, xgb_best.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "Stacking is an ensemble machine learning method that can classify data with results from different classifiers and a training result within itself. The model emerged by the results from different models and the result of training within itself is called the meta-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(C=10, gamma=0.01, kernel='rbf', probability=True, random_state=42)\n",
    "rf_model = RandomForestClassifier(max_depth=5, min_samples_split=8, n_estimators=108, random_state=42)\n",
    "xgb_model = XGBClassifier(n_estimators=200, learning_rate=0.01, max_depth=3, min_child_weight=10, random_state=42)\n",
    "\n",
    "# Apply feature selection to each base model\n",
    "sfs_svm = SequentialFeatureSelector(svm_model, n_features_to_select=5, direction='forward', cv=5)\n",
    "sfs_rf = SequentialFeatureSelector(rf_model, n_features_to_select=5, direction='forward', cv=5)\n",
    "sfs_xgb = SequentialFeatureSelector(xgb_model, n_features_to_select=5, direction='forward', cv=5)\n",
    "\n",
    "# Fit feature selectors and transform the data\n",
    "X_train_sfs_svm = sfs_svm.fit_transform(X_train_scaled, y_train)\n",
    "X_train_sfs_rf = sfs_rf.fit_transform(X_train_scaled, y_train)\n",
    "X_train_sfs_xgb = sfs_xgb.fit_transform(X_train_scaled, y_train)\n",
    "\n",
    "# Define base models for stacking with feature selection\n",
    "base_models = [\n",
    "    ('svm', Pipeline([('selector', sfs_svm), ('model', svm_model)])),\n",
    "    ('rf', Pipeline([('selector', sfs_rf), ('model', rf_model)])),\n",
    "    ('xgb', Pipeline([('selector', sfs_xgb), ('model', xgb_model)]))\n",
    "]\n",
    "\n",
    "# Define meta-model (using Logistic Regression as the final estimator)\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Create the stacking classifier\n",
    "stacking_classifier = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    stack_method='predict_proba'\n",
    ")\n",
    "\n",
    "# Fit the stacking classifier\n",
    "stacking_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the stacking model\n",
    "print(\"\\nStacking Model Performance:\")\n",
    "y_pred_stacking = stacking_classifier.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred_stacking))\n",
    "stacking_accuracy = accuracy_score(y_test, y_pred_stacking)\n",
    "print(f\"Accuracy Score: {stacking_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve Comparison\n",
    "The ROC curve across models shows that Random Forest with no feature selection has the highest AUC at 0.84 while Stacking RF has relatively the poorest AUC at 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [rf_best, rf_sfs_best, xgb_best, stacking_model]\n",
    "model_names = ['Random Forest (No FS)', 'Random Forest (With FS)', 'XGBoost', 'Stacking RF']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for model, name in zip(models, model_names):\n",
    "    if name == 'Random Forest (With FS)':\n",
    "        y_pred_proba = model.predict_proba(X_test_sfs)[:, 1]\n",
    "    else:\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC={auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices\n",
    "The confusion matrix shows that Random Forest with Feature Selection and XGBoost perform the best at identifying True Positives and True Negatives but struggle with False Negatives and False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, X_test, y_test, title):\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    if name == 'Random Forest (With FS)':\n",
    "        plot_confusion_matrix(model, X_test_sfs, y_test, f\"Confusion Matrix - {name}\")\n",
    "    else:\n",
    "        plot_confusion_matrix(model, X_test_scaled, y_test, f\"Confusion Matrix - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy comparison\n",
    "The model accuracy comparison shows that stacking is clearly the most powerful model but the confusion matrix and the low AUC score shows that the model is struggling to differentiate between True and False Positives.These results can be explained by the outcome imbalance in the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [\n",
    "    accuracy_score(y_test, rf_best.predict(X_test_scaled)) * 100,\n",
    "    accuracy_score(y_test, rf_sfs_best.predict(X_test_sfs)) * 100,\n",
    "    accuracy_score(y_test, xgb_best.predict(X_test_scaled)) * 100,\n",
    "    accuracy_score(y_test, stacking_classifier.predict(X_test_scaled)) * 100 \n",
    "]\n",
    "\n",
    "# Plot model accuracies as percentages\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model_names, accuracies)\n",
    "plt.title('Model Accuracy Comparison (in %)')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(0, 100)  \n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i, v - 2, f'{v:.2f}%', ha='center', va='bottom', fontsize=10) \n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion and Conclusion\n",
    "\n",
    "Based on the results, it is clear that Stacking RF and other boosting techinques provides a better accuracy than standard Random Forest classifiers. However, it also points to the *importance of preprocessing* the data to achieve higher levels of discrimination between different outcomes. Without enough preprocessing of the data, models tend to learn less and end up relying on majority votes to predict outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Kaina Zhao and Zhiping Wang. 2024. Research on Diabetes Prediction Based on Machine Learning. In Proceedings of the 6th International Conference on Machine Learning and Machine Intelligence (MLMI '23). Association for Computing Machinery, New York, NY, USA, 29–33. https://doi-org.colorado.idm.oclc.org/10.1145/3635638.3635643\n",
    "\n",
    "Xi Li, Michele Curiger, Rolf Dornberger, and Thomas Hanne. 2023. Optimized Computational Diabetes Prediction with Feature Selection Algorithms. In Proceedings of the 2023 7th International Conference on Intelligent Systems, Metaheuristics & Swarm Intelligence (ISMSI '23). Association for Computing Machinery, New York, NY, USA, 36–43. https://doi-org.colorado.idm.oclc.org/10.1145/3596947.3596948\n",
    "\n",
    "Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '16). Association for Computing Machinery, New York, NY, USA, 785–794. https://doi-org.colorado.idm.oclc.org/10.1145/2939672.2939785"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
